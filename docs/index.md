# Re-Imagining Access

## Introduction

This page documents the process of installing and testing an audio transcription demonstration application based on the open-source automatic speech recognition (ASR) system called [Whisper](https://openai.com/blog/whisper/), released on September 21, 2022 by OpenAI.  

Whisper is a deep neural network trained on a large (680,000 hours of recordings) and diverse dataset collected from the web.  It represents a new milestone in speech recognition with "improved robustness to accents, background noise and technical language" among other advertised features.

## Background

The effort to research and investigate an easy to deploy practical ASR application was supported by a grant from the [Institute of Museum and Library Sciences](https://www.imls.gov/) (IMLS) as part of the “Re-Imagining Access” project carried out by the members of faculty, staff and students at the ArtCenter College of Design.

During the summer of 2021, students from the studio course instructed by Elise Co, Robert Dirig, Josh Halstead, & Todd Masilko proposed the "Double Check" prototype concept as part of an answer to a question: "How might we design an archive system that ensures accessibility from beginning to end?"  The term accessibility in this context refers to the need to make archive resources designed for individuals with disabilities in mind.

The students identified a list of potential use cases and corresponding capability requirements, rated according to their priority for the accessibility checker prototype concept.  Automatic transcription of archived audio and video files was identified as one high priority item on this list.  This accessibility feature was subsequently chosen as a focus of an investigation into a potential functional solution that could in principle be integrated as a feature of the archival systems employed by ArtCenter and other institutions.
